import codecs
import dill
from typing import TypeAlias, Optional, Literal, TypeVar, Generic, cast
from tree_sitter import Language, Parser, Tree, Point

from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import create_react_agent

from beaker_kernel.lib.agent import BeakerAgent
from beaker_kernel.lib.tools import tool
from beaker_kernel.lib.config import config
from beaker_kernel.lib.utils import DefaultModel
from .analyzer import AnalysisCategory
from .analysis_types import AnalysisAnnotation, AnalysisIssue


class AnalysisResult(BaseModel):
    """
    Representation of a lint-style analysis that will be generated by an LLM agent and passed to the code_analysis tool.
    """
    issue_type: str = Field(description=(
        "Unique ID indicating the issue type represented. This value should always match one of the issue types "
        "provided in the prompt instructions."
    ))
    analysis_rule_id: str = Field(description="Unique ID indicating which of the provided rules this analysis falls under.")
    extra_info: Optional[str] = Field(
        description=(
            "Optional extra information that may be useful to the user to understand the analysis, particularly in cases "
            "where the issue or rule is vague or potentially confusing."
        ),
        default=None,
    )
    cell_id: Optional[str] = Field(
        description=(
            "The ID of the cell the code is generated from, if available from the context."
        ),
        default=None,
    )
    code_start_line: int = Field(description="Starting line number for which the analysis applies.")
    code_end_line: int = Field(description=(
        "Ending line number for which the analysis applies. If the analysis is for a single line, this value should be "
        "the same as the value for `code_start_line`."
    ))
    code_start_line_pos: Optional[int] = Field(
        default=None,
        description=(
            "Optional position in start line where analysis starts. If not provided, defaults to start of starting line."
    ))
    code_end_line_pos: Optional[int] = Field(
        default=None,
        description=(
            "Optional position in end line where analysis ends. If not provided, defaults to end of ending line."
    ))


class AnalysisAgent(BeakerAgent):
    """
    You are a code evaluation agent, whose job it is to evaluate code that is generated by a LLM and to provide helpful
    information so that they can more easily identify potential issues with the code and to understand the code and what
    it is doing more efficiently.

    You should always run the tool code_analysis, providing it with the results of your analysis of the code provided in
    the context.
    This tool will format the analysis and present it to the user.
    """.strip()

    def __init__(
        self, *, model = None, api_key = None, tools = None, max_errors = 3, max_react_steps = None,
        thought_handler = ..., messages = None, custom_prelude = None,
        code: str = None,
        issues: list[AnalysisIssue] = None,
        beaker_kernel = None,
        **kwargs
    ):
        # Create the code analysis tool
        code_analysis_tool = self._create_code_analysis_tool()
        all_tools = [code_analysis_tool] + (tools or [])
        
        # Set up model
        agent_model = model or config.get_model() or DefaultModel({})
        
        super().__init__(
            context=None,
            tools=all_tools,
            **kwargs
        )
        
        self.beaker_kernel = beaker_kernel
        self._stop_requested = False
        self._analysis_result = None
        
        # Add code to chat history if provided
        if code is not None:
            system_message = HumanMessage(content=f"""\
Code to run rules against:
```
{code}
```
""")
            self.chat_history.add_message(system_message)
            
        issues = issues if isinstance(issues, list) else []
        self.issues = [issue for issue in issues if hasattr(issue, 'prompt_description')]

    def thought_callback(self, thought, tool_name, tool_input):
        if self.beaker_kernel:
            self.beaker_kernel.log(
                event_type="code_analysis_thought",
                content={
                    "thought": thought,
                    "tool_name": tool_name,
                    "tool_input": tool_input,
                }
            )

    @property
    def instruction_prompt(self):
        content: list[str] = [
            """\
 Below are the types of annotations/issues that you should use when analyzing code and generating analyses.
 All generated analyses should match exactly one of these rules.
 """
        ]
        for issue in self.issues:
            content.append(
                f"""\
## {issue.title} ({issue.id})
    id: {issue.id}
    severity: {issue.severity}
    description: \\
```
{issue.prompt_description}
```"""
            )
        return "\n".join(content)

    def _create_code_analysis_tool(self):
        """Create the code analysis tool bound to this agent instance."""
        
        @tool
        def code_analysis(analysis_list: list[dict]) -> str:
            """
            Given the code and rules above, please inspect the code for matching issues. For each issue identified, please
            generate an Analysis object

            Include all instances in the code that a rule applies to. It is fine to include the same line(s) more than once
            if more than one rule applies to that code segment.

            If successful, this tool will immediately stop the react loop and return the analyses to the invoking process.

            Arguments:
                analysis_list: A list of analysis objects indicating the type and location of the issue.
            Returns:
                str: Confirmation message that analysis is complete.
            """
            # Convert dict representations back to AnalysisResult objects
            analysis_objects = []
            for item in analysis_list:
                if isinstance(item, dict):
                    analysis_objects.append(AnalysisResult(**item))
                else:
                    analysis_objects.append(item)
            
            # Store the result in the agent instance
            self._analysis_result = analysis_objects
            self._stop_requested = True
            
            output = dill.dumps(analysis_objects)
            # Return must be a string, and dill.dumps returns bytes so decode appropriately
            encoded_result = codecs.encode(output, 'base64').decode()
            
            return f"Analysis complete. Found {len(analysis_objects)} issues. Result encoded and stored."
        
        return code_analysis
    
    async def get_analysis_result(self):
        """Get the stored analysis result after execution."""
        return self._analysis_result
